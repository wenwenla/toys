{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "print(env.action_space)\n",
    "print(env.observation_space.low, env.observation_space.high)\n",
    "print(env.action_space.sample())\n",
    "print(env.step(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "action = {\n",
    "    'left': 0,\n",
    "    'stop': 1,\n",
    "    'right': 2,\n",
    "}\n",
    "R = []\n",
    "t = 0\n",
    "rm = -1\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    if t < 30:\n",
    "        state, reward, done, info = env.step(action['left'])\n",
    "    elif t < 50:\n",
    "        state, reward, done, info = env.step(action['right'])\n",
    "    elif t < 80:\n",
    "        state, reward, done, info = env.step(action['left'])\n",
    "    else:\n",
    "        state, reward, done, info = env.step(action['right'])\n",
    "    if done:\n",
    "        print('Done')\n",
    "        break\n",
    "    rm = max(rm, state[0])\n",
    "    t += 1\n",
    "    time.sleep(0.05)\n",
    "print(rm)\n",
    "print(R)\n",
    "time.sleep(5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(state):\n",
    "    return (round(state[0], 2), round(state[1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_qtable():\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = env.reset()\n",
    "print(st)\n",
    "print(get_state(st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = 4096 * 1000\n",
    "Alpha = 0.5\n",
    "Gamma = 0.9\n",
    "Epsilon = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(qtable, state):\n",
    "    row = qtable.get(state, [0, 0, 0])\n",
    "    mx = max(row)\n",
    "    res = []\n",
    "    for index in range(len(row)):\n",
    "        if row[index] == mx:\n",
    "            res.append(index)\n",
    "    return random.choice(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_eps(qtable, state, epsilon):\n",
    "    if random.random() <= epsilon or state not in qtable:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return select(qtable, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_learning\n",
    "qtable = init_qtable()\n",
    "for _ in range(Episode):\n",
    "    st = get_state(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        act = select_eps(qtable, st, Epsilon)\n",
    "        state, reward, done, info = env.step(act)\n",
    "        new_st = get_state(state)\n",
    "        new_act = select(qtable, new_st)\n",
    "        if st not in qtable:\n",
    "            qtable[st] = [0, 0, 0]\n",
    "        if new_st not in qtable:\n",
    "            qtable[new_st] = [0, 0, 0]\n",
    "        qtable[st][act] += Alpha * (reward + Gamma * qtable[new_st][new_act] - qtable[st][act])\n",
    "        st = new_st\n",
    "        act = new_act\n",
    "        \n",
    "        if _ % 1024 == 0:\n",
    "            env.render()\n",
    "            time.sleep(0.05)\n",
    "        \n",
    "        if done and len(info) == 0:\n",
    "            print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa\n",
    "qtable = init_qtable()\n",
    "for _ in range(Episode):\n",
    "    st = get_state(env.reset())\n",
    "    act = select(qtable, st)\n",
    "    done = False\n",
    "    #print('Episode {}'.format(_))\n",
    "\n",
    "    while not done:\n",
    "        state, reward, done, info = env.step(act)\n",
    "        \n",
    "        nst = get_state(state)\n",
    "        nact = select(qtable, nst)\n",
    "        qtable[st][act] += Alpha * (reward + Gamma * qtable[nst][nact] - qtable[st][act]) \n",
    "        \n",
    "        st = nst\n",
    "        act = nact\n",
    "            \n",
    "        if done and len(info) == 0:\n",
    "            print('Ok')\n",
    "            \n",
    "        if _ % 4096 == 0:\n",
    "            pass\n",
    "#             env.render()\n",
    "#             time.sleep(0.05)\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
